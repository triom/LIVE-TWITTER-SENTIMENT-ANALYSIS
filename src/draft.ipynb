{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\learning\\LIVE-TWITTER-SENTIMENT-ANALYSIS\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pyodbc\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Connection string example\n",
    "conn = pyodbc.connect(\n",
    "    'DRIVER={SQL Server};'\n",
    "    'SERVER=DESKTOP-8JEBTPG;'  # Specify the server name\n",
    "    'DATABASE=tweets;'  # Specify the database name\n",
    "    'UID=sa;'  # Specify your username\n",
    "    'PWD=Triomphe&14;'  # Specify your password\n",
    ")\n",
    "\n",
    "# Create a cursor from the connection\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Example: execute a query\n",
    "cursor.execute(\"SELECT * FROM tweet\")\n",
    "\n",
    "# Fetch all rows from the executed query\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Print rows\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_count</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>Created At</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>England</td>\n",
       "      <td>Our fans. Our players. Our summer.\\n\\nThis is ...</td>\n",
       "      <td>Thu Jun 06 18:00:20 +0000 2024</td>\n",
       "      <td>4543</td>\n",
       "      <td>26633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tommy Robinson 🇬🇧</td>\n",
       "      <td>Man dies at Yorkshire dales Hotspot at the wee...</td>\n",
       "      <td>Tue Aug 13 14:55:10 +0000 2024</td>\n",
       "      <td>1391</td>\n",
       "      <td>9769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>England Extra</td>\n",
       "      <td>For the people mocking Lee Carsley being a pos...</td>\n",
       "      <td>Tue Jul 16 17:25:57 +0000 2024</td>\n",
       "      <td>433</td>\n",
       "      <td>4542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CentreGoals.</td>\n",
       "      <td>🚨🚨| Manchester United are monitoring Eberechi ...</td>\n",
       "      <td>Tue Sep 10 11:24:17 +0000 2024</td>\n",
       "      <td>80</td>\n",
       "      <td>1817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>England</td>\n",
       "      <td>Ready for a big summer! 📸\\n\\n#ThreeLions | @ma...</td>\n",
       "      <td>Mon Jun 10 20:24:12 +0000 2024</td>\n",
       "      <td>2011</td>\n",
       "      <td>22827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet_count           Username  \\\n",
       "0            1            England   \n",
       "1            2  Tommy Robinson 🇬🇧   \n",
       "2            3      England Extra   \n",
       "3            4       CentreGoals.   \n",
       "4            5            England   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Our fans. Our players. Our summer.\\n\\nThis is ...   \n",
       "1  Man dies at Yorkshire dales Hotspot at the wee...   \n",
       "2  For the people mocking Lee Carsley being a pos...   \n",
       "3  🚨🚨| Manchester United are monitoring Eberechi ...   \n",
       "4  Ready for a big summer! 📸\\n\\n#ThreeLions | @ma...   \n",
       "\n",
       "                       Created At  Retweets  Likes  \n",
       "0  Thu Jun 06 18:00:20 +0000 2024      4543  26633  \n",
       "1  Tue Aug 13 14:55:10 +0000 2024      1391   9769  \n",
       "2  Tue Jul 16 17:25:57 +0000 2024       433   4542  \n",
       "3  Tue Sep 10 11:24:17 +0000 2024        80   1817  \n",
       "4  Mon Jun 10 20:24:12 +0000 2024      2011  22827  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('tweets_1.csv') \n",
    "df1[\"Text\"] = df1[\"Text\"].astype(str)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('tweets_2.csv') \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11394"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.319327731092437"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11394/2142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2['tweet_date'] = pd.to_datetime(df2['tweet_date'], format='%a %b %d %H:%M:%S %z %Y')\n",
    "df2['tweet_date'] = df2['tweet_date'].dt.strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Created At'] = pd.to_datetime(df1['Created At'], format='%a %b %d %H:%M:%S %z %Y')\n",
    "df1['Created At'] = df1['Created At'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "df1 = df1.drop(columns=[\"Tweet_count\" ,\"Username\", \"Retweets\", \"Likes\"])\n",
    "df1.rename(columns={'Text': 'tweet_content', 'Created At':'tweet_date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tweet_location  tweet_date  \\\n",
       "0                  NYC  02-03-2020   \n",
       "1          Seattle, WA  02-03-2020   \n",
       "2                  NaN  02-03-2020   \n",
       "3          Chicagoland  02-03-2020   \n",
       "4  Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                       tweet_content  \n",
       "0  TRENDING: New Yorkers encounter empty supermar...  \n",
       "1  When I couldn't find hand sanitizer at Fred Me...  \n",
       "2  Find out how you can protect yourself and love...  \n",
       "3  #Panic buying hits #NewYork City as anxious sh...  \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning and prep\n",
    "def data_cleaning_and_preparation(df, text):\n",
    "    # 1. Drop rows with missing values in text columns\n",
    "    df.dropna(subset=[text], inplace=True)\n",
    "\n",
    "    # 2. Remove duplicates\n",
    "    df.drop_duplicates(subset=[text], inplace=True)\n",
    "\n",
    "    # 3. Convert text to lowercase\n",
    "    df[text] = df[text].str.lower()\n",
    "\n",
    "    # 4. Remove special characters, punctuation, and numimbers\n",
    "    df[text] = df[text].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
    "\n",
    "    # 5. Remove extra whitespace\n",
    "    df[text] = df[text].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "\n",
    "    # 7. Check the cleaned data\n",
    "    print(df.head())\n",
    "    return df\n",
    "    # Save the cleaned text data\n",
    "    # df.to_csv('cleaned_text_data.csv', index=False)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = data_cleaning_and_preparation(df2, text='tweet_content')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# List to store processed DataFrames\n",
    "processed_dfs = []\n",
    "\n",
    "# Split the DataFrame into 5 equal parts\n",
    "split_dfs = np.array_split(df2, 5)\n",
    "\n",
    "for df_part in split_dfs:\n",
    "    # Apply sentiment analysis\n",
    "    sentiment_result = sentiment_analysis_transformers(df_part, text='tweet_content')\n",
    "    \n",
    "    # Apply hashtag generator and drop \"scores\" column\n",
    "    df_with_hashtags = hashtag_generator(df_part, text_column='tweet_content')\n",
    "    df_with_hashtags = df_with_hashtags.drop(columns=[\"scores\"])\n",
    "    \n",
    "    # Add unique IDs and set empty 'tweet_location'\n",
    "    df_with_hashtags['id_dist'] = [uuid.uuid4() for _ in range(len(df_with_hashtags))]\n",
    "    df_with_hashtags['tweet_location'] = \"\"\n",
    "    \n",
    "    # Append the processed DataFrame to the list\n",
    "    processed_dfs.append(df_with_hashtags)\n",
    "\n",
    "# Concatenate all processed DataFrames into a single DataFrame\n",
    "final_df = pd.concat(processed_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_transformers(df, text):\n",
    "    \n",
    "    model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\"\n",
    "    analyser = pipeline(\"sentiment-analysis\", model=model_path)\n",
    "    df['scores'] = df[text].apply(lambda text: analyser (text))\n",
    "    df['Sentiment'] = df['scores'].apply(lambda output: output[0]['label']) \n",
    "    df['sentiment_score'] = df['scores'].apply(lambda output: output[0]['score']) \n",
    "    df['Sentiment'] = df['Sentiment'].apply(lambda x: 'Positive ❤️' if x == 'positive' else ('Negative 😡' if x == 'negative' else 'Neutral 💛')) \n",
    "    df = df.drop(columns = [\"scores\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "\n",
    "\n",
    "def hashtag_generator(df, text_column):\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"fabiochiu/t5-base-tag-generation\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"fabiochiu/t5-base-tag-generation\")\n",
    "    \n",
    "    # Function to generate hashtags for a single text\n",
    "    def generate_tags(text):\n",
    "        if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "            return []  # Return an empty list for empty or missing text\n",
    "        \n",
    "        # Tokenize and generate hashtags using the model\n",
    "        inputs = tokenizer([text], max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "        output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=10, max_length=64)\n",
    "        decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "        tags = list(set(decoded_output.strip().split(\", \")))  # Get unique hashtags\n",
    "        return tags\n",
    "    \n",
    "    # Apply the generate_tags function to each row of the text column\n",
    "    df['hashtag'] = df[text_column].apply(generate_tags)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate hashtags for the 'text' column\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "\n",
    "def hashtag_generator(text):\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"fabiochiu/t5-base-tag-generation\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"fabiochiu/t5-base-tag-generation\")\n",
    "    \n",
    "    # Function to generate hashtags for a single text\n",
    "    def generate_tags(text):\n",
    "        if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "            return []  # Return an empty list for empty or missing text\n",
    "        \n",
    "        # Tokenize and generate hashtags using the model\n",
    "        inputs = tokenizer([text], max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "        output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=10, max_length=64)\n",
    "        decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "        tags = list(set(decoded_output.strip().split(\", \")))  # Get unique hashtags\n",
    "        return tags\n",
    "\n",
    "    return generate_tags(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\beaut\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['World', 'Europe', 'Football']\n"
     ]
    }
   ],
   "source": [
    "text = \"our fans our players our summer this is your threelions squad announcement video for euro httpstcoqugnzgsp\"\n",
    "hashtags = hashtag_generator(text)\n",
    "print(hashtags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\learning\\LIVE-TWITTER-SENTIMENT-ANALYSIS\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sentiment_result = sentiment_analysis_transformers(df2, text='tweet_content')\n",
    "df_with_hashtags = hashtag_generator(df2, text_column='tweet_content')\n",
    "df_with_hashtags =  df_with_hashtags.drop(columns = [\"scores\"])\n",
    "df_with_hashtags['id_dist'] = [uuid.uuid4() for _ in range(len(df_with_hashtags))]\n",
    "df_with_hashtags['tweet_location'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign unique IDs\n",
    "\n",
    "\n",
    "# Generate a unique ID for each row\n",
    "df_with_hashtags['id_dist'] = [uuid.uuid4() for _ in range(len(df_with_hashtags))]\n",
    "df_with_hashtags['tweet_location'] = \"\"\n",
    "\n",
    "\n",
    "# df_with_hashtags['id_dist'] = range(1, len(df_with_hashtags) + 1)\n",
    "# df_with_hashtags['tweet_location'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>trending new yorkers encounter empty supermark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>when i couldnt find hand sanitizer at fred mey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>find out how you can protect yourself and love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>panic buying hits newyork city as anxious shop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>toiletpaper dunnypaper coronavirus coronavirus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>Israel ??</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>meanwhile in a supermarket in israel people da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3794</th>\n",
       "      <td>Farmington, NM</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>did you panic buy a lot of nonperishable items...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>Haverford, PA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>asst prof of economics cconces was on nbcphila...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>gov need to do somethings instead of biar je r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>Arlington, Virginia</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>i and forestandpaper members are committed to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3798 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_location  tweet_date  \\\n",
       "0                     NYC  02-03-2020   \n",
       "1             Seattle, WA  02-03-2020   \n",
       "2                     NaN  02-03-2020   \n",
       "3             Chicagoland  02-03-2020   \n",
       "4     Melbourne, Victoria  03-03-2020   \n",
       "...                   ...         ...   \n",
       "3793            Israel ??  16-03-2020   \n",
       "3794       Farmington, NM  16-03-2020   \n",
       "3795        Haverford, PA  16-03-2020   \n",
       "3796                  NaN  16-03-2020   \n",
       "3797  Arlington, Virginia  16-03-2020   \n",
       "\n",
       "                                          tweet_content  \n",
       "0     trending new yorkers encounter empty supermark...  \n",
       "1     when i couldnt find hand sanitizer at fred mey...  \n",
       "2     find out how you can protect yourself and love...  \n",
       "3     panic buying hits newyork city as anxious shop...  \n",
       "4     toiletpaper dunnypaper coronavirus coronavirus...  \n",
       "...                                                 ...  \n",
       "3793  meanwhile in a supermarket in israel people da...  \n",
       "3794  did you panic buy a lot of nonperishable items...  \n",
       "3795  asst prof of economics cconces was on nbcphila...  \n",
       "3796  gov need to do somethings instead of biar je r...  \n",
       "3797  i and forestandpaper members are committed to ...  \n",
       "\n",
       "[3798 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\learning\\LIVE-TWITTER-SENTIMENT-ANALYSIS\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "d:\\learning\\LIVE-TWITTER-SENTIMENT-ANALYSIS\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "d:\\learning\\LIVE-TWITTER-SENTIMENT-ANALYSIS\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "d:\\learning\\LIVE-TWITTER-SENTIMENT-ANALYSIS\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "d:\\learning\\LIVE-TWITTER-SENTIMENT-ANALYSIS\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "d:\\learning\\LIVE-TWITTER-SENTIMENT-ANALYSIS\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# List to store processed DataFrames\n",
    "processed_dfs = []\n",
    "\n",
    "# Split the DataFrame into 5 equal parts\n",
    "split_dfs = np.array_split(df2, 5)\n",
    "\n",
    "for df_part in split_dfs:\n",
    "    # Apply sentiment analysis\n",
    "    sentiment_result = sentiment_analysis_transformers(df_part, text='tweet_content')\n",
    "    \n",
    "    # Apply hashtag generator and drop \"scores\" column\n",
    "    df_with_hashtags = hashtag_generator(df_part, text_column='tweet_content')\n",
    "    df_with_hashtags = df_with_hashtags.drop(columns=[\"scores\"])\n",
    "    \n",
    "    # Add unique IDs and set empty 'tweet_location'\n",
    "    df_with_hashtags['id_dist'] = [uuid.uuid4() for _ in range(len(df_with_hashtags))]\n",
    "        \n",
    "    # Append the processed DataFrame to the list\n",
    "    processed_dfs.append(df_with_hashtags)\n",
    "\n",
    "# Concatenate all processed DataFrames into a single DataFrame\n",
    "final_df = pd.concat(processed_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>tweet_content</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>id_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>trending new yorkers encounter empty supermark...</td>\n",
       "      <td>Negative 😡</td>\n",
       "      <td>0.500374</td>\n",
       "      <td>[Virus, New York City, Coronavirus]</td>\n",
       "      <td>1891c43a-d795-493a-9d35-dacc73992983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>when i couldnt find hand sanitizer at fred mey...</td>\n",
       "      <td>Negative 😡</td>\n",
       "      <td>0.874354</td>\n",
       "      <td>[Virus, Covid, Covid 19, Coronavirus]</td>\n",
       "      <td>90453505-3e57-4f75-9347-8df4a49ec6a1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>find out how you can protect yourself and love...</td>\n",
       "      <td>Positive ❤️</td>\n",
       "      <td>0.800672</td>\n",
       "      <td>[Virus, Covid, Covid 19, Coronavirus]</td>\n",
       "      <td>b2ad8e08-c504-428a-a8eb-319379e94022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>panic buying hits newyork city as anxious shop...</td>\n",
       "      <td>Negative 😡</td>\n",
       "      <td>0.898224</td>\n",
       "      <td>[Virus, New York City, Coronavirus]</td>\n",
       "      <td>39301481-881a-4bab-8556-cc263a12ee93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>toiletpaper dunnypaper coronavirus coronavirus...</td>\n",
       "      <td>Neutral 💛</td>\n",
       "      <td>0.584825</td>\n",
       "      <td>[Covid 19, Self, Health, Virus, Mental Health,...</td>\n",
       "      <td>5b8f2f04-fe2d-4920-b4a3-b986d2f91985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3793</th>\n",
       "      <td>Israel ??</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>meanwhile in a supermarket in israel people da...</td>\n",
       "      <td>Neutral 💛</td>\n",
       "      <td>0.664129</td>\n",
       "      <td>[Virus, Covid, Covid 19, Coronavirus]</td>\n",
       "      <td>a7be15cb-2e1e-4e8e-a1e0-816e09cf73d1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3794</th>\n",
       "      <td>Farmington, NM</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>did you panic buy a lot of nonperishable items...</td>\n",
       "      <td>Neutral 💛</td>\n",
       "      <td>0.678648</td>\n",
       "      <td>[Food, Volunteering, Covid 19, Foodies, Covid ...</td>\n",
       "      <td>46693621-5500-4cbb-9d6b-34d999c6b717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3795</th>\n",
       "      <td>Haverford, PA</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>asst prof of economics cconces was on nbcphila...</td>\n",
       "      <td>Neutral 💛</td>\n",
       "      <td>0.745936</td>\n",
       "      <td>[Economics, Virus, Economy, Tech, Technology, ...</td>\n",
       "      <td>c9ac011a-8005-47e9-9333-cb51adc4d03f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3796</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>gov need to do somethings instead of biar je r...</td>\n",
       "      <td>Negative 😡</td>\n",
       "      <td>0.919379</td>\n",
       "      <td>[Covid 19, Virus, Covid 19 Covid 19 Covid 19 C...</td>\n",
       "      <td>c000158e-ba97-4552-8627-33bda0ab371a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3797</th>\n",
       "      <td>Arlington, Virginia</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>i and forestandpaper members are committed to ...</td>\n",
       "      <td>Positive ❤️</td>\n",
       "      <td>0.612763</td>\n",
       "      <td>[Finance, Investment, Economics, Economy, Inve...</td>\n",
       "      <td>5a9e9581-41c9-4de7-aef6-e4bb1bc0ead9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3798 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet_location  tweet_date  \\\n",
       "0                     NYC  02-03-2020   \n",
       "1             Seattle, WA  02-03-2020   \n",
       "2                     NaN  02-03-2020   \n",
       "3             Chicagoland  02-03-2020   \n",
       "4     Melbourne, Victoria  03-03-2020   \n",
       "...                   ...         ...   \n",
       "3793            Israel ??  16-03-2020   \n",
       "3794       Farmington, NM  16-03-2020   \n",
       "3795        Haverford, PA  16-03-2020   \n",
       "3796                  NaN  16-03-2020   \n",
       "3797  Arlington, Virginia  16-03-2020   \n",
       "\n",
       "                                          tweet_content    Sentiment  \\\n",
       "0     trending new yorkers encounter empty supermark...   Negative 😡   \n",
       "1     when i couldnt find hand sanitizer at fred mey...   Negative 😡   \n",
       "2     find out how you can protect yourself and love...  Positive ❤️   \n",
       "3     panic buying hits newyork city as anxious shop...   Negative 😡   \n",
       "4     toiletpaper dunnypaper coronavirus coronavirus...    Neutral 💛   \n",
       "...                                                 ...          ...   \n",
       "3793  meanwhile in a supermarket in israel people da...    Neutral 💛   \n",
       "3794  did you panic buy a lot of nonperishable items...    Neutral 💛   \n",
       "3795  asst prof of economics cconces was on nbcphila...    Neutral 💛   \n",
       "3796  gov need to do somethings instead of biar je r...   Negative 😡   \n",
       "3797  i and forestandpaper members are committed to ...  Positive ❤️   \n",
       "\n",
       "      sentiment_score                                            hashtag  \\\n",
       "0            0.500374                [Virus, New York City, Coronavirus]   \n",
       "1            0.874354              [Virus, Covid, Covid 19, Coronavirus]   \n",
       "2            0.800672              [Virus, Covid, Covid 19, Coronavirus]   \n",
       "3            0.898224                [Virus, New York City, Coronavirus]   \n",
       "4            0.584825  [Covid 19, Self, Health, Virus, Mental Health,...   \n",
       "...               ...                                                ...   \n",
       "3793         0.664129              [Virus, Covid, Covid 19, Coronavirus]   \n",
       "3794         0.678648  [Food, Volunteering, Covid 19, Foodies, Covid ...   \n",
       "3795         0.745936  [Economics, Virus, Economy, Tech, Technology, ...   \n",
       "3796         0.919379  [Covid 19, Virus, Covid 19 Covid 19 Covid 19 C...   \n",
       "3797         0.612763  [Finance, Investment, Economics, Economy, Inve...   \n",
       "\n",
       "                                   id_dist  \n",
       "0     1891c43a-d795-493a-9d35-dacc73992983  \n",
       "1     90453505-3e57-4f75-9347-8df4a49ec6a1  \n",
       "2     b2ad8e08-c504-428a-a8eb-319379e94022  \n",
       "3     39301481-881a-4bab-8556-cc263a12ee93  \n",
       "4     5b8f2f04-fe2d-4920-b4a3-b986d2f91985  \n",
       "...                                    ...  \n",
       "3793  a7be15cb-2e1e-4e8e-a1e0-816e09cf73d1  \n",
       "3794  46693621-5500-4cbb-9d6b-34d999c6b717  \n",
       "3795  c9ac011a-8005-47e9-9333-cb51adc4d03f  \n",
       "3796  c000158e-ba97-4552-8627-33bda0ab371a  \n",
       "3797  5a9e9581-41c9-4de7-aef6-e4bb1bc0ead9  \n",
       "\n",
       "[3798 rows x 7 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a column as index, then reset the index\n",
    "df_with_hashtags = final_df.set_index('id_dist')  # Set 'id_dist' as the index temporarily\n",
    "\n",
    "# Reset the index without the 'id_dist' column\n",
    "df_with_hashtags = df_with_hashtags.reset_index(drop=False)\n",
    "\n",
    "# Move 'id_dist' to the end of the DataFrame\n",
    "df_with_hashtags = df_with_hashtags[['tweet_content', 'tweet_date', 'tweet_location', 'hashtag', 'sentiment_score', 'Sentiment', 'id_dist']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_content</th>\n",
       "      <th>tweet_date</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>id_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trending new yorkers encounter empty supermark...</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>NYC</td>\n",
       "      <td>Virus</td>\n",
       "      <td>0.500374</td>\n",
       "      <td>Negative 😡</td>\n",
       "      <td>1891c43a-d795-493a-9d35-dacc73992983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trending new yorkers encounter empty supermark...</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>NYC</td>\n",
       "      <td>New York City</td>\n",
       "      <td>0.500374</td>\n",
       "      <td>Negative 😡</td>\n",
       "      <td>1891c43a-d795-493a-9d35-dacc73992983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trending new yorkers encounter empty supermark...</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>NYC</td>\n",
       "      <td>Coronavirus</td>\n",
       "      <td>0.500374</td>\n",
       "      <td>Negative 😡</td>\n",
       "      <td>1891c43a-d795-493a-9d35-dacc73992983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when i couldnt find hand sanitizer at fred mey...</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Virus</td>\n",
       "      <td>0.874354</td>\n",
       "      <td>Negative 😡</td>\n",
       "      <td>90453505-3e57-4f75-9347-8df4a49ec6a1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when i couldnt find hand sanitizer at fred mey...</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Covid</td>\n",
       "      <td>0.874354</td>\n",
       "      <td>Negative 😡</td>\n",
       "      <td>90453505-3e57-4f75-9347-8df4a49ec6a1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tweet_content  tweet_date  \\\n",
       "0  trending new yorkers encounter empty supermark...  02-03-2020   \n",
       "1  trending new yorkers encounter empty supermark...  02-03-2020   \n",
       "2  trending new yorkers encounter empty supermark...  02-03-2020   \n",
       "3  when i couldnt find hand sanitizer at fred mey...  02-03-2020   \n",
       "4  when i couldnt find hand sanitizer at fred mey...  02-03-2020   \n",
       "\n",
       "  tweet_location        hashtag  sentiment_score   Sentiment  \\\n",
       "0            NYC          Virus         0.500374  Negative 😡   \n",
       "1            NYC  New York City         0.500374  Negative 😡   \n",
       "2            NYC    Coronavirus         0.500374  Negative 😡   \n",
       "3    Seattle, WA          Virus         0.874354  Negative 😡   \n",
       "4    Seattle, WA          Covid         0.874354  Negative 😡   \n",
       "\n",
       "                                id_dist  \n",
       "0  1891c43a-d795-493a-9d35-dacc73992983  \n",
       "1  1891c43a-d795-493a-9d35-dacc73992983  \n",
       "2  1891c43a-d795-493a-9d35-dacc73992983  \n",
       "3  90453505-3e57-4f75-9347-8df4a49ec6a1  \n",
       "4  90453505-3e57-4f75-9347-8df4a49ec6a1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explode the hashtags column\n",
    "final_1 = df_with_hashtags.explode('hashtag')\n",
    "\n",
    "# Reset the index for a cleaner output\n",
    "final = final_1.reset_index(drop=True)\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_content       object\n",
       "tweet_date          object\n",
       "tweet_location     float64\n",
       "hashtag             object\n",
       "sentiment_score    float64\n",
       "Sentiment           object\n",
       "id_dist             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = pd.read_csv(\"result.csv\")\n",
    "result1 = result1.drop(columns=result1.columns[0])  # Drop the first column\n",
    "result1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beaut\\AppData\\Local\\Temp\\ipykernel_30016\\1998775306.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  result1['sentiment_score'].fillna(0.0, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tweet_content       object\n",
       "tweet_date          object\n",
       "tweet_location      object\n",
       "hashtag             object\n",
       "sentiment_score    float64\n",
       "Sentiment           object\n",
       "id_dist             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1['sentiment_score'] = result1['sentiment_score'].apply(lambda x: round(float(x), 2))\n",
    "# Replace NaN with a default value if any are found\n",
    "result1['sentiment_score'].fillna(0.0, inplace=True)\n",
    "# Convert to float to ensure compatibility\n",
    "result1['sentiment_score'] = result1['sentiment_score'].astype(float)\n",
    "result1['tweet_location'] = result1['tweet_location'].astype(str)\n",
    "result1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1['tweet_location'] = result1['tweet_location'].fillna('Unknown')  # or leave as None depending on your preference\n",
    "\n",
    "# 3. Ensure sentiment_score is float (it should already be float64)\n",
    "result1['sentiment_score'] = result1['sentiment_score'].astype(float)\n",
    "\n",
    "# 4. Ensure id_dist is valid UUID (string)\n",
    "result1['id_dist'] = result1['id_dist'].apply(lambda x: str(uuid.UUID(x)) if pd.notna(x) else None)\n",
    "\n",
    "# 5. Ensure text columns are string type\n",
    "result1['tweet_content'] = result1['tweet_content'].astype(str)\n",
    "result1['hashtag'] = result1['hashtag'].astype(str)\n",
    "result1['Sentiment'] = result1['Sentiment'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "\n",
    "# Connection string example\n",
    "conn = pyodbc.connect(\n",
    "    'DRIVER={SQL Server};'\n",
    "    'SERVER=DESKTOP-8JEBTPG;'  # Specify the server name\n",
    "    'DATABASE=tweets;'  # Specify the database name\n",
    "    'UID=sa;'  # Specify your username\n",
    "    'PWD=Triomphe&14;'  # Specify your password\n",
    ")\n",
    "\n",
    "# Create a cursor from the connection\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL INSERT query\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO [dbo].[tweet] \n",
    "           ([tweet_content], [tweet_date], [tweet_location], [hashtag], [sentiment_score], [sentiment], [id_dist])\n",
    "     VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\"\n",
    "\n",
    "# Iterate over DataFrame rows and insert data into SQL table\n",
    "for index, row in result1.iterrows():\n",
    "    cursor.execute(insert_query,\n",
    "                   row['tweet_content'], \n",
    "                   row['tweet_date'], \n",
    "                   row['tweet_location'], \n",
    "                   row['hashtag'], \n",
    "                   row['sentiment_score'], \n",
    "                   row['Sentiment'],\n",
    "                   row['id_dist'])\n",
    "\n",
    "# Commit the transaction\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = pd.read_csv(\"result_2.csv\")\n",
    "\n",
    "result2['tweet_location'] = result2['tweet_location'].fillna('Unknown')  # or leave as None depending on your preference\n",
    "result2['tweet_date'] = result2['tweet_date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "# 3. Ensure sentiment_score is float (it should already be float64)\n",
    "result2['sentiment_score'] = result2['sentiment_score'].astype(float)\n",
    "\n",
    "# 4. Ensure id_dist is valid UUID (string)\n",
    "result2['id_dist'] = result2['id_dist'].apply(lambda x: str(uuid.UUID(x)) if pd.notna(x) else None)\n",
    "\n",
    "# 5. Ensure text columns are string type\n",
    "result2['tweet_content'] = result2['tweet_content'].astype(str)\n",
    "result2['hashtag'] = result2['hashtag'].astype(str)\n",
    "result2['Sentiment'] = result2['Sentiment'].astype(str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'tweet_date' to datetime\n",
    "result2['tweet_date'] = pd.to_datetime(result2['tweet_date'], errors='coerce')\n",
    "\n",
    "# Fill any NaT values with a default date if needed (e.g., '1900-01-01')\n",
    "result2['tweet_date'].fillna(pd.Timestamp('1900-01-01'), inplace=True)\n",
    "\n",
    "# Verify the data type of 'tweet_date'\n",
    "print(result2['tweet_date'].dtype)  # Should output datetime64[ns]\n",
    "print(result2[['tweet_date']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ensure 'sentiment_score' is a valid float and handle NaN values by replacing them with np.nan\n",
    "result2['sentiment_score'] = result2['sentiment_score'].fillna(np.nan).astype(float)\n",
    "\n",
    "# Ensure 'tweet_date' is a valid datetime and handle any conversion issues\n",
    "result2['tweet_date'] = pd.to_datetime(result2['tweet_date'], errors='coerce')\n",
    "\n",
    "# Fill any missing values in 'tweet_location' with 'Unknown' (or None if you prefer)\n",
    "result2['tweet_location'] = result2['tweet_location'].fillna('Unknown')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# Connection string example\n",
    "conn = pyodbc.connect(\n",
    "    'DRIVER={SQL Server};'\n",
    "    'SERVER=DESKTOP-8JEBTPG;'  # Specify the server name\n",
    "    'DATABASE=tweets;'  # Specify the database name\n",
    "    'UID=sa;'  # Specify your username\n",
    "    'PWD=Triomphe&14;'  # Specify your password\n",
    ")\n",
    "\n",
    "# Create a cursor from the connection\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL INSERT query\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO [dbo].[tweet] \n",
    "           ([tweet_content], [tweet_date], [tweet_location], [hashtag], [sentiment_score], [sentiment], [id_dist])\n",
    "     VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Iterate over DataFrame rows and insert data into SQL table\n",
    "for index, row in result2.iterrows():\n",
    "    cursor.execute(insert_query,\n",
    "                   row['tweet_content'], \n",
    "                   row['tweet_date'], \n",
    "                   row['tweet_location'], \n",
    "                   row['hashtag'], \n",
    "                   row['sentiment_score'], \n",
    "                   row['Sentiment'],\n",
    "                   row['id_dist'])\n",
    "\n",
    "# Commit the transaction\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('result_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_Sent1 = pd.read_csv(\"result.csv\")\n",
    "df_Sent2 = pd.read_csv(\"result_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "\n",
    "# Function to get the dominant emotion\n",
    "def get_dominant_emotion(text):\n",
    "    # Set up the emotion classifier pipeline\n",
    "    classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=False)    \n",
    "    result = classifier(text)\n",
    "    return result[0]['label']  # Return only the dominant emotion label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'confusion', 'score': 0.9057261943817139}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=\"tasinhoque/roberta-large-go-emotions\", return_all_scores=False)\n",
    "print(classifier(\"I have no idea about this issue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row in the DataFrame\n",
    "df_Sent1['emotion'] = df_Sent1['tweet_content'].apply(get_dominant_emotion)\n",
    "\n",
    "# Display the DataFrame with the new 'dominant_emotion' column\n",
    "# print(df_Sent1)\n",
    "df_Sent1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           tweet_content  tweet_date  \\\n",
      "0      trending new yorkers encounter empty supermark...  02-03-2020   \n",
      "1      trending new yorkers encounter empty supermark...  02-03-2020   \n",
      "2      trending new yorkers encounter empty supermark...  02-03-2020   \n",
      "3      when i couldnt find hand sanitizer at fred mey...  02-03-2020   \n",
      "4      when i couldnt find hand sanitizer at fred mey...  02-03-2020   \n",
      "...                                                  ...         ...   \n",
      "19700  i and forestandpaper members are committed to ...  16-03-2020   \n",
      "19701  i and forestandpaper members are committed to ...  16-03-2020   \n",
      "19702  i and forestandpaper members are committed to ...  16-03-2020   \n",
      "19703  i and forestandpaper members are committed to ...  16-03-2020   \n",
      "19704  i and forestandpaper members are committed to ...  16-03-2020   \n",
      "\n",
      "            tweet_location           hashtag  sentiment_score    Sentiment  \\\n",
      "0                      NYC             Virus         0.500374   Negative 😡   \n",
      "1                      NYC     New York City         0.500374   Negative 😡   \n",
      "2                      NYC       Coronavirus         0.500374   Negative 😡   \n",
      "3              Seattle, WA             Virus         0.874354   Negative 😡   \n",
      "4              Seattle, WA             Covid         0.874354   Negative 😡   \n",
      "...                    ...               ...              ...          ...   \n",
      "19700  Arlington, Virginia         Investing         0.612763  Positive ❤️   \n",
      "19701  Arlington, Virginia            Saving         0.612763  Positive ❤️   \n",
      "19702  Arlington, Virginia      Saving Money         0.612763  Positive ❤️   \n",
      "19703  Arlington, Virginia  Money Management         0.612763  Positive ❤️   \n",
      "19704  Arlington, Virginia        Management         0.612763  Positive ❤️   \n",
      "\n",
      "                                    id_dist  emotion  \n",
      "0      1891c43a-d795-493a-9d35-dacc73992983     fear  \n",
      "1      1891c43a-d795-493a-9d35-dacc73992983     fear  \n",
      "2      1891c43a-d795-493a-9d35-dacc73992983     fear  \n",
      "3      90453505-3e57-4f75-9347-8df4a49ec6a1  sadness  \n",
      "4      90453505-3e57-4f75-9347-8df4a49ec6a1  sadness  \n",
      "...                                     ...      ...  \n",
      "19700  5a9e9581-41c9-4de7-aef6-e4bb1bc0ead9      joy  \n",
      "19701  5a9e9581-41c9-4de7-aef6-e4bb1bc0ead9      joy  \n",
      "19702  5a9e9581-41c9-4de7-aef6-e4bb1bc0ead9      joy  \n",
      "19703  5a9e9581-41c9-4de7-aef6-e4bb1bc0ead9      joy  \n",
      "19704  5a9e9581-41c9-4de7-aef6-e4bb1bc0ead9      joy  \n",
      "\n",
      "[19705 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to each row in the DataFrame\n",
    "df_Sent2['emotion'] = df_Sent2['tweet_content'].apply(get_dominant_emotion)\n",
    "\n",
    "# Display the DataFrame with the new 'dominant_emotion' column\n",
    "print(df_Sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert 'tweet_date' to datetime\n",
    "df_Sent2['tweet_date'] = pd.to_datetime(df_Sent2['tweet_date'], format='%d-%m-%Y')\n",
    "\n",
    "# Normalize the datetime values to set time to 00:00:00\n",
    "df_Sent2['tweet_date'] = df_Sent2['tweet_date'].apply(lambda x: x.normalize())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Sent1.to_csv('result_emotion_1.csv', index=False)\n",
    "df_Sent2.to_csv('result_emotion_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_Sent2 = pd.read_csv(\"result_emotion_2.csv\")\n",
    "df_Sent1 = pd.read_csv(\"result_emotion_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Sent1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Sent2['tweet_location'] = df_Sent2['tweet_location'].fillna('Unknown')\n",
    "df_Sent2['tweet_date'] = pd.to_datetime(df_Sent2['tweet_date'], errors='coerce')\n",
    "df_Sent2['sentiment_score'] = df_Sent2['sentiment_score'].fillna(0).round(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during insertion: ('42000', '[42000] [Microsoft][ODBC SQL Server Driver][SQL Server]The incoming tabular data stream (TDS) remote procedure call (RPC) protocol stream is incorrect. Parameter 6 (\"\"): The supplied value is not a valid instance of data type float. Check the source data for invalid values. An example of an invalid value is data of numeric type with scale greater than precision. (8023) (SQLExecDirectW)')\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# Connection string example\n",
    "conn = pyodbc.connect(\n",
    "    'DRIVER={SQL Server};'\n",
    "    'SERVER=DESKTOP-8JEBTPG;'  # Specify the server name\n",
    "    'DATABASE=tweets;'  # Specify the database name\n",
    "    'UID=sa;'  # Specify your username\n",
    "    'PWD=Triomphe&14;'  # Specify your password\n",
    ")\n",
    "\n",
    "# Create a cursor from the connection\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL INSERT query\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO [dbo].[tweet] \n",
    "           ([tweet_content], [tweet_date], [tweet_location], [hashtag], [sentiment_score], [sentiment], [id_dist], [emotion])\n",
    "     VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Now try inserting the data again\n",
    "try:\n",
    "    for index, row in df_Sent2.iterrows():\n",
    "        cursor.execute(insert_query,\n",
    "                       row['tweet_content'], \n",
    "                       row['tweet_date'], \n",
    "                       row['tweet_location'], \n",
    "                       row['hashtag'], \n",
    "                       row['sentiment_score'], \n",
    "                       row['Sentiment'],\n",
    "                       row['id_dist'],\n",
    "                       row['emotion'])\n",
    "    conn.commit()  # Commit if no exceptions occur\n",
    "except Exception as e:\n",
    "    print(f\"Error during insertion: {e}\")\n",
    "\n",
    "# Finally, close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de l'insertion à l'index 1842 : ('42000', '[42000] [Microsoft][ODBC SQL Server Driver][SQL Server]The incoming tabular data stream (TDS) remote procedure call (RPC) protocol stream is incorrect. Parameter 7 (\"\"): The supplied value is not a valid instance of data type float. Check the source data for invalid values. An example of an invalid value is data of numeric type with scale greater than precision. (8023) (SQLExecDirectW)')\n",
      "Erreur lors de l'insertion à l'index 6648 : ('42000', '[42000] [Microsoft][ODBC SQL Server Driver][SQL Server]The incoming tabular data stream (TDS) remote procedure call (RPC) protocol stream is incorrect. Parameter 7 (\"\"): The supplied value is not a valid instance of data type float. Check the source data for invalid values. An example of an invalid value is data of numeric type with scale greater than precision. (8023) (SQLExecDirectW)')\n",
      "Erreur lors de l'insertion à l'index 14274 : ('42000', '[42000] [Microsoft][ODBC SQL Server Driver][SQL Server]The incoming tabular data stream (TDS) remote procedure call (RPC) protocol stream is incorrect. Parameter 7 (\"\"): The supplied value is not a valid instance of data type float. Check the source data for invalid values. An example of an invalid value is data of numeric type with scale greater than precision. (8023) (SQLExecDirectW)')\n",
      "Erreur lors de l'insertion à l'index 17185 : ('42000', '[42000] [Microsoft][ODBC SQL Server Driver][SQL Server]The incoming tabular data stream (TDS) remote procedure call (RPC) protocol stream is incorrect. Parameter 7 (\"\"): The supplied value is not a valid instance of data type float. Check the source data for invalid values. An example of an invalid value is data of numeric type with scale greater than precision. (8023) (SQLExecDirectW)')\n",
      "Erreur lors de l'insertion à l'index 18918 : ('42000', '[42000] [Microsoft][ODBC SQL Server Driver][SQL Server]The incoming tabular data stream (TDS) remote procedure call (RPC) protocol stream is incorrect. Parameter 7 (\"\"): The supplied value is not a valid instance of data type float. Check the source data for invalid values. An example of an invalid value is data of numeric type with scale greater than precision. (8023) (SQLExecDirectW)')\n",
      "Erreur lors de l'insertion à l'index 19103 : ('42000', '[42000] [Microsoft][ODBC SQL Server Driver][SQL Server]The incoming tabular data stream (TDS) remote procedure call (RPC) protocol stream is incorrect. Parameter 7 (\"\"): The supplied value is not a valid instance of data type float. Check the source data for invalid values. An example of an invalid value is data of numeric type with scale greater than precision. (8023) (SQLExecDirectW)')\n",
      "Erreur lors de l'insertion à l'index 19566 : ('42000', '[42000] [Microsoft][ODBC SQL Server Driver][SQL Server]The incoming tabular data stream (TDS) remote procedure call (RPC) protocol stream is incorrect. Parameter 7 (\"\"): The supplied value is not a valid instance of data type float. Check the source data for invalid values. An example of an invalid value is data of numeric type with scale greater than precision. (8023) (SQLExecDirectW)')\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration de la connexion SQL Server\n",
    "conn = pyodbc.connect(\n",
    "    'DRIVER={SQL Server};'\n",
    "    'SERVER=DESKTOP-8JEBTPG;'\n",
    "    'DATABASE=tweets;'\n",
    "    'UID=sa;'\n",
    "    'PWD=Triomphe&14;'\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL INSERT query\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO [dbo].[tweet] \n",
    "           ([tweet_content], [tweet_date], [tweet_location], [hashtag], [sentiment_score], [sentiment], [id_dist], [emotion])\n",
    "     VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "\"\"\"\n",
    "\n",
    "# Assurer que `sentiment_score` est bien float64 et sans NaN\n",
    "df_Sent2['sentiment_score'] = pd.to_numeric(df_Sent2['sentiment_score'], errors='coerce')\n",
    "df_Sent2['sentiment_score'] = df_Sent2['sentiment_score'].fillna(0).astype('float64')  # Remplace NaN par 0 si nécessaire\n",
    "\n",
    "# Tentative d'insertion avec gestion d'erreurs détaillée\n",
    "try:\n",
    "    for index, row in df_Sent2.iterrows():\n",
    "        try:\n",
    "            # Exécutez la commande d'insertion pour chaque ligne\n",
    "            cursor.execute(insert_query,\n",
    "                           row['tweet_content'], \n",
    "                           row['tweet_date'], \n",
    "                           row['tweet_location'], \n",
    "                           row['hashtag'], \n",
    "                           row['sentiment_score'],  # Assurez-vous que c'est un float64\n",
    "                           row['Sentiment'],\n",
    "                           row['id_dist'],\n",
    "                           row['emotion'])\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'insertion à l'index {index} : {e}\")\n",
    "    conn.commit()  # Commit si aucune exception\n",
    "except Exception as e:\n",
    "    print(f\"Erreur globale pendant l'insertion : {e}\")\n",
    "\n",
    "# Fermeture de la connexion\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
